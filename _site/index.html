<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Xiaoxiao Ma - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Xiaoxiao Ma">
<meta property="og:title" content="Xiaoxiao Ma">


  <link rel="canonical" href="http://localhost:4000/">
  <meta property="og:url" content="http://localhost:4000/">



  <meta property="og:description" content="">









<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <!-- <div class="masthead__menu"> -->
      <!-- <nav id="site-nav" class="greedy-nav"> -->
        <!-- <button><div class="navicon"></div></button> -->
        <!-- <ul class="visible-links"> -->
          <!-- <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li> -->
          <!--  -->
        <!-- </ul> -->
        <!-- <ul class="hidden-links hidden"></ul> -->
      <!-- </nav> -->
    <!-- </div> -->
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/myphoto.jpg" class="author__avatar" alt="Xiaoxiao Ma">
  </div>

  <div class="author__content">
    <h3 class="author__name">Xiaoxiao Ma</h3>
    <p class="author__bio">University of Science and Technology of China</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;"></div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Anhui, China</li>
      
      
      
      
        <li><a href="mailto:xiao_xiao@mail.ustc.edu.cn"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=EE_KGzcAAAAJ&hl=en"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="mailto:xiao_xiao@mail.ustc.edu.cn"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com/citations?user=EE_KGzcAAAAJ&hl=en"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            
<p><span class="anchor" id="about-me"></span></p>

<p><br /><br /></p>
<h1 id="-about-me">üëÄ About Me</h1>

<p>Hi,</p>

<p>üå± I‚Äôm Xiaoxiao Ma, a first-year PhD student at USTC, in <a href="https://bivlab123.github.io/">USTC-BIVLab</a> supervised by <a href="https://scholar.google.com/citations?user=r6CvuOUAAAAJ">Prof. Feng Zhao</a>. I am currently a research intern at Meituan</p>

<p>üìñ My research interest includes:</p>
<ul>
  <li>Generative models &amp; image synthesis, autoregressive models, vision-language models</li>
  <li>Image restoration, image enhancement</li>
</ul>

<p>üì´ Looking forward to any collaborations or internship positions, feel free to contact me via email</p>

<p><br /><br /></p>
<h1 id="-news">üî• News</h1>
<ul>
  <li><em>2025.09</em>: ¬† Delighted to announce that <a href="https://neurips.cc/virtual/2025/poster/118537">ARSample</a> was accepted by NeurIPS 2025!</li>
  <li><em>2025.06</em>: ¬† Delighted to announce that <a href="https://arxiv.org/abs/2507.22431">HQ-CLIP</a> was accepted by ICCV 2025!</li>
  <li><em>2024.09</em>: ¬† Delighted to announce that <a href="https://arxiv.org/abs/2401.14966">MPI</a> was accepted by NeurIPS 2024!</li>
  <li><em>2024.09</em>: ¬†  I was invited to give a talk at ByteDance as the author of <a href="https://arxiv.org/abs/2406.10797">STAR</a>! See slides <a href="https://github.com/krennic999/krennic999.github.io/blob/main/files/star_pre_talk.pdf">here</a></li>
  <li><em>2024.06</em>: ¬† <a href="https://arxiv.org/abs/2406.10797">STAR</a> was released on Arxiv.</li>
</ul>

<p><br /><br /></p>
<h1 id="-publications">üìù Publications</h1>

<!-- ----------------------------------------------------------- -->

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">Arxiv 2024</div><img src="images/STAR.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2406.10797">STAR: Scale-wise Text-to-image generation via Auto-Regressive representations</a></p>
    <iframe src="https://ghbtns.com/github-btn.html?user=krennic999&amp;repo=STAR&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>

    <p><strong>Xiaoxiao Ma*</strong>, Mohan Zhou*, Tao Liang, Yalong Bai, et al.</p>

    <p><a href="https://krennic999.github.io/STAR/"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="EE_KGzcAAAAJ:UeHWp8X0CEIC"></span></strong></p>
    <ul>
      <li>STAR is a novel scale-wise text-to-image model that is effective and efficient in performance</li>
      <li>Notably, STAR shows efficiency by requiring 2.95s to generate 512√ó512 images (compared to 6.48s for PixArt-Œ±)</li>
    </ul>
  </div>
</div>

<!-- ----------------------------------------------------------- -->

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">NeurIPS 2025</div><img src="images/ARsample.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://neurips.cc/virtual/2025/poster/118537">Towards Better &amp; Faster Autoregressive Image Generation: From the Perspective of Entropy</a></p>
    <iframe src="https://ghbtns.com/github-btn.html?user=krennic999&amp;repo=ARsample&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>

    <p><strong>Xiaoxiao Ma</strong>, Feng Zhao, Pengyang Ling, Haibo Qiu, et al.</p>

    <p><a href="https://github.com/krennic999/ARsample"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="EE_KGzcAAAAJ:ufrVoPGSRksC"></span></strong></p>
    <ul>
      <li>We revisit the sampling problem in autoregressive image generation and reveal the low and uneven information density of image tokens.</li>
      <li>Based on this insight, we propose an entropy-informed decoding strategy that improves both generation quality and efficiency across diverse AR models and benchmarks.</li>
    </ul>
  </div>
</div>

<!-- ----------------------------------------------------------- -->

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">Arxiv 2025</div><img src="images/STAGE.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2509.25027">STAGE: Stable and Generalizable GRPO for Autoregressive Image Generation</a></p>
    <iframe src="https://ghbtns.com/github-btn.html?user=krennic999&amp;repo=STAGE&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>

    <p><strong>Xiaoxiao Ma</strong>, Haibo Qiu, Guohui Zhang, Zhixiong Zeng, et al.</p>

    <p><a href="https://github.com/krennic999/STAGE"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="EE_KGzcAAAAJ:ufrVoPGSRksC"></span></strong></p>
    <ul>
      <li>STAGE is the first system to study the stability and generalization of GRPO-based autoregressive visual generation.</li>
      <li>Built upon Janus-Pro-7B, STAGE improves the GenEval score from 0.78 to 0.89 (‚âà14%) without compromising image quality, and its effectiveness generalizes well across benchmarks such as T2I-Compbench and ImageReward.</li>
    </ul>
  </div>
</div>

<!-- ----------------------------------------------------------- -->

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">NeurIPS 2024</div><img src="images/MPI.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2401.14966">Masked Pre-trained Model Enables Universal Zero-shot Denoiser</a></p>
    <iframe src="https://ghbtns.com/github-btn.html?user=krennic999&amp;repo=MPI&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>

    <p><strong>Xiaoxiao Ma*</strong>, Zhixiang Wei*, Yi Jin, Pengyang Ling, et al.</p>

    <p><a href="https://github.com/krennic999/MPI"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="EE_KGzcAAAAJ:9yKSN-GCB0IC"></span></strong></p>
    <ul>
      <li>MPI is a zero-shot denoising pipeline designed for many types of noise degradations</li>
      <li>Only around 10s takes for a MPI to denoise on single noisy image</li>
    </ul>
  </div>
</div>

<!-- ----------------------------------------------------------- -->

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ICCV 2025</div><img src="images/hqclip.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2507.22431">HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models</a></p>
    <iframe src="https://ghbtns.com/github-btn.html?user=w1oves&amp;repo=hqclip&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>

    <p>Zhixiang Wei*, Guangting Wang*, <strong>Xiaoxiao Ma</strong>, et al.</p>

    <p><a href="https://zxwei.site/hqclip/"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="EE_KGzcAAAAJ:YsMSGLbcyi4C"></span></strong></p>
    <ul>
      <li>A CLIP training framework trained on 1.3B bidirectional image‚Äìtext pairs, combining bidirectional supervision and label classification, achieving SoTA zero-shot and retrieval performance.</li>
    </ul>
  </div>
</div>

<!-- ----------------------------------------------------------- -->

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">CVPR 2024</div><img src="images/Reins.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2312.04265">Stronger, Fewer, \&amp; Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation</a></p>
    <iframe src="https://ghbtns.com/github-btn.html?user=w1oves&amp;repo=Rein&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>

    <p>Zhixiang Wei*, Lin Chen*, Yi Jin*, <strong>Xiaoxiao Ma</strong>, et al.</p>

    <p><a href="https://github.com/w1oves/Rein"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="EE_KGzcAAAAJ:2osOgNQ5qMEC"></span></strong></p>
    <ul>
      <li>Rein is a PEFT framework based on vision foundation models for domain generalized semantic segmentation (DGSS) with merely 1% trainable parameters</li>
    </ul>
  </div>
</div>

<!-- ----------------------------------------------------------- -->

<p><br /><br /></p>
<h1 id="-experiences">üíª Experiences</h1>
<ul>
  <li><em>2025.04 - Persent</em>, Meituan, Beijing.</li>
  <li><em>2024.12 - 2025.03</em>, Shanghai AI Laboratory, Shanghai.</li>
  <li><em>2024.04 - 2024.12</em>, Duxiaoman, Beijing.</li>
</ul>

<p><br /><br /></p>
<h1 id="-academic-service-reviewer">üìù Academic Service (Reviewer)</h1>
<ul>
  <li>ICLR 2026</li>
  <li>NeurIPS 2025, 2026</li>
  <li>IEEE TPAMI</li>
  <li>IEEE JBHI</li>
</ul>

<p><br /><br /></p>
<h1 id="-honors-and-awards">üéñ Honors and Awards</h1>
<ul>
  <li>2022~2025 First Prize Scholarship of USTC for four continusous years</li>
  <li>2024 National Scholarship for Undergraduate Students</li>
</ul>

<p><br /><br /></p>
<h1 id="-educations">üìñ Educations</h1>
<ul>
  <li><em>2025.09 - now</em>, University of Science and Technology of China, Anhui, PhD candidate in Multimodal Learning</li>
  <li><em>2022.09 - 2025.06</em>, University of Science and Technology of China, Anhui, Master candidate in Computer Vision</li>
  <li><em>2018.09 - 2022.06</em>, China Agricultural University, Beijing. B. Eng in Computer Science</li>
</ul>

<p><br /><br /></p>

<!-- # üí¨ Invited Talks
- *2021.06*, Xiaoxiao Ma dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Xiaoxiao Ma dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)-->

          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://cdn.jsdelivr.net/gh/RayeRen/acad-homepage.github.io@'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  </body>
</html>
